{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUATQkrG7VPh",
        "outputId": "3fb11b42-203e-47de-9577-dcf7f60b46e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Suyashi:[0.38922439 0.35189901 0.84105034]\n",
            "is:[0.92315899 0.71718402 0.58548672]\n",
            "happy:[0.29746199 0.78964488 0.46794944]\n"
          ]
        }
      ],
      "source": [
        "#Word Embedding in Python\n",
        "import numpy as np\n",
        "\n",
        "#Define the vocabulary\n",
        "vocab={\"Suyashi\":0,\"is\":1,\"happy\":2}\n",
        "\n",
        "#One hot encoding\n",
        "def one_hot_encoding(word_index,vocab_size):\n",
        "  one_hot=np.zeros(vocab_size)\n",
        "  one_hot[word_index]=1\n",
        "  return one_hot\n",
        "\n",
        "#Define weight matrix(Random initialization for simplicity)\n",
        "weight_matrix=np.random.rand(len(vocab),3)\n",
        "\n",
        "#Multiply one-hot encoded words with weight matrix\n",
        "word_embeddings={}\n",
        "for word,index in vocab.items():\n",
        "  one_hot_encoded=one_hot_encoding(index,len(vocab))\n",
        "  embedding=np.dot(one_hot_encoded,weight_matrix)\n",
        "  word_embeddings[word]=embedding\n",
        "\n",
        "#Display Word Embedding\n",
        "for word,embedding in word_embeddings.items():\n",
        "  print(f\"{word}:{embedding}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Position Encoding\n",
        "def position_encoding(sentence_length,embedding_dim):\n",
        "  position_encodings=np.zeros((sentence_length,embedding_dim))\n",
        "  for pos in range(sentence_length):\n",
        "    for i in range(embedding_dim):\n",
        "      if i%2==0:\n",
        "        position_encodings[pos,i]=np.sin(pos/(10000**(i/embedding_dim)))\n",
        "      else:\n",
        "        position_encodings[pos,i]=np.cos(pos/(10000**(i/embedding_dim)))\n",
        "  return position_encodings\n",
        "\n",
        "#Assuming word Embedding for the word as\n",
        "word_embeddings={\n",
        "    \"Suyashi\":np.array([0.1,0.2,0.3]),\n",
        "    \"is\":np.array([0.2,0.3,0.1]),\n",
        "    \"happy\":np.array([0.3,0.2,0.1])\n",
        "}\n",
        "\n",
        "#Get positional encoding\n",
        "sentence_length=3\n",
        "embedding_dim=3\n",
        "pos_encoding=position_encoding(sentence_length,embedding_dim)\n",
        "\n",
        "#Add positional encodings to word embedding\n",
        "for i,word in enumerate(word_embeddings):\n",
        "  word_embeddings[word]+=pos_encoding[i%sentence_length]\n",
        "\n",
        "#Display word embeddings with positional encoding\n",
        "print(\"Word embeddings with positional encodings: \")\n",
        "for word,embedding in word_embeddings.items():\n",
        "  print(f\"{word}:{embedding}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pvYm6wl-F1l",
        "outputId": "974a5608-2815-43f9-d3b6-d1e7450876af"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word embeddings with positional encodings: \n",
            "Suyashi:[0.1 1.2 0.3]\n",
            "is:[1.04147098 1.29892298 0.10215443]\n",
            "happy:[1.20929743 1.19569422 0.10430886]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment for different inputs:\n",
        "import numpy as np\n",
        "vocab={\"Harsh\":0,\"is\":1,\"good\":2}\n",
        "def one_hot_encoding(word_index,vocab_size):\n",
        "  one_hot=np.zeros(vocab_size)\n",
        "  one_hot[word_index]=1\n",
        "  return one_hot\n",
        "weight_matrix=np.random.rand(len(vocab),3)\n",
        "word_embeddings={}\n",
        "for word,index in vocab.items():\n",
        "  one_hot_encoded=one_hot_encoding(index,len(vocab))\n",
        "  embedding=np.dot(one_hot_encoded,weight_matrix)\n",
        "  word_embeddings[word]=embedding\n",
        "for word,embedding in word_embeddings.items():\n",
        "  print(f\"{word}:{embedding}\")\n",
        "''' First word output: This is a new World\n",
        "This:[0.37928376 0.67816009 0.09586772]\n",
        "is:[0.51017103 0.28390923 0.65922563]\n",
        "a:[0.32178089 0.0931155  0.59146445]\n",
        "new:[0.30693361 0.65274464 0.04086934]\n",
        "world:[0.62303239 0.87452204 0.92181557]\n",
        "'''\n",
        "'''\n",
        "Second Word Output: Surya is intelligent\n",
        "Surya:[0.28010313 0.93321526 0.25804402]\n",
        "is:[0.36446093 0.48190762 0.65824799]\n",
        "intelligent:np.array([0.84740405 0.7197824 0.91840574]\n",
        "'''\n",
        "'''\n",
        "Fourth Word Output: Time Flies fast\n",
        "Time:[0.37057976 0.49285592 0.70951577]\n",
        "flies:[0.56796562 0.13268141 0.34161634]\n",
        "fast:[0.53242533 0.48253626 0.42947063]\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "nqVyHQFl-F4x",
        "outputId": "87ac888e-5dec-428b-f83c-16cedb9650d9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Harsh:[0.16871054 0.35683662 0.87450374]\n",
            "is:[0.06804776 0.47985298 0.21414211]\n",
            "good:[0.18081712 0.68324533 0.69925216]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nFourth Word Output: Time Flies fast\\nTime:[0.37057976 0.49285592 0.70951577]\\nflies:[0.56796562 0.13268141 0.34161634]\\nfast:[0.53242533 0.48253626 0.42947063]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Position Encoding\n",
        "def position_encoding(sentence_length,embedding_dim):\n",
        "  position_encodings=np.zeros((sentence_length,embedding_dim))\n",
        "  for pos in range(sentence_length):\n",
        "    for i in range(embedding_dim):\n",
        "      if i%2==0:\n",
        "        position_encodings[pos,i]=np.sin(pos/(10000**(i/embedding_dim)))\n",
        "      else:\n",
        "        position_encodings[pos,i]=np.cos(pos/(10000**(i/embedding_dim)))\n",
        "  return position_encodings\n",
        "\n",
        "#Assuming word Embedding for the word as\n",
        "word_embeddings={\n",
        "\"Time\":np.array([0.37057976,0.49285592,0.70951577]),\n",
        "\"flies\":np.array([0.56796562,0.13268141,0.34161634]),\n",
        "\"fast\":np.array([0.53242533,0.48253626,0.42947063])\n",
        "}\n",
        "#Get positional encoding\n",
        "sentence_length=3\n",
        "embedding_dim=3\n",
        "pos_encoding=position_encoding(sentence_length,embedding_dim)\n",
        "\n",
        "#Add positional encodings to word embedding\n",
        "for i,word in enumerate(word_embeddings):\n",
        "  word_embeddings[word]+=pos_encoding[i%sentence_length]\n",
        "\n",
        "#Display word embeddings with positional encoding\n",
        "print(\"Word embeddings with positional encodings: \")\n",
        "for word,embedding in word_embeddings.items():\n",
        "  print(f\"{word}:{embedding}\")\n",
        "  '''\n",
        "  Word embeddings with positional encodings: This is a new world\n",
        "This:[0.37928376 1.67816009 0.09586772]\n",
        "is:[1.35164201 1.28283221 0.66138006]\n",
        "a:[1.23107832 1.08880972 0.59577331]\n",
        "new:[0.44805362 1.64306534 0.0473326 ]\n",
        "world:[-0.13377011  1.85733602  0.9304332 ]\n",
        "  '''\n",
        "  '''\n",
        "Word embeddings with positional encodings: Surya is intelligent\n",
        "Surya:[0.28010313 1.93321526 0.25804402]\n",
        "is:[1.20593191 1.4808306  0.66040242]\n",
        "intelligent:[1.75670148 1.71547662 0.9227146 ]\n",
        "  '''\n",
        "  '''\n",
        "Word embeddings with positional encodings: Time Flies fast\n",
        "Time:[0.37057976 1.49285592 0.70951577]\n",
        "flies:[1.4094366  1.13160439 0.34377077]\n",
        "fast:[1.44172276 1.47823048 0.43377949]\n",
        "  '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCntEGbPDx5K",
        "outputId": "d97ca4c6-a1da-4c83-9ee8-a3e7139d78a4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word embeddings with positional encodings: \n",
            "Time:[0.37057976 1.49285592 0.70951577]\n",
            "flies:[1.4094366  1.13160439 0.34377077]\n",
            "fast:[1.44172276 1.47823048 0.43377949]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B8fAn14sDyAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qNWu0_SXDyES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MCRuudKvDyHs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}